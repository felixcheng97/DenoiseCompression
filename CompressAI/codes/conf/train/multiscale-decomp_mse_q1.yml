#### general settings

name: multiscale-decomp_mse_q1
use_tb_logger: true
gpu_ids: [0]
# manual_seed: 1

#### datasets

datasets:
  train:
    root: ../../data/flicker
    name: synthetic
    patch_size: 256
    batch_size: 16
    n_workers: 4
    use_shuffle: true
  val:
    root: ../../data/flicker
    name: synthetic
    patch_size: 256
    batch_size: 1
    n_workers: 0
    use_shuffle: false
  
#### network

network:
  model: multiscale-decomp
  quality: 1
  criterions:
    criterion_metric: mse
    criterion_fea: l1
  lambdas: 
    lambda_metric: 0.0018
    lambda_fea: 3.
  alphas:
    alpha_metric: 1.
    alpha_fea: 1.
  pretrained: true
  
#### training settings

train:
  mode: epoch
  epoch:
    value: 600
    lr: !!float 1e-4
    lr_aux: !!float 1e-3
    lr_scheme: LambdaLR
    warm_up_counts: 20
    milestones: [450, 550] 
    gamma: 0.1
    val_freq: 1 # number of epochs to start a visualization
  clip_max_norm: 1.0
  loss_cap: 100

#### path
path:
  root: ~/
  # checkpoint: ../../experiments/multiscale-decomp_mse_q1/checkpoints/checkpoint_best_loss.pth.tar
  
#### logger

logger:
  print_freq: 100 # number of iterations to print a training progress
  save_checkpoint_freq: 10  # number of epochs to save a checkpoint